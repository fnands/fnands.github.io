{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c210b514",
   "metadata": {},
   "source": [
    "---\n",
    "layout: post\n",
    "title: Stereo vision and disparity maps (in Julia)\n",
    "categories: [julia, coding, stereo]\n",
    "excerpt: An introduction into basic stereo vision, with a simple block matching algorithm written in Julia. \n",
    "---\n",
    "\n",
    "I've been working a lot recently with stereo vision and wanted to go through the basics of how disparity is calculated. I'm partially doing this as an excuse to get better at Julia (v1.9.3).  \n",
    "\n",
    "## Introduction\n",
    "\n",
    "In much the same way that we as humans can have depth perception by sensing the difference in the images we see between our left and right eyes, we can calculate depth from a pair of images taken from different locations, called a stereo pair.   \n",
    "\n",
    "If we know the positions of out cameras, then we can use matching points in our two images to estimate how far away from the camera those points are. \n",
    "\n",
    "Taking a look at the image below (from [OpenCV](https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html)):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dabbddc1-1b71-43b2-9533-9f308bf665ed",
   "metadata": {},
   "source": [
    "![https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html](../images/dispairity_block_julia/stereo_depth.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76d6dea4-25a3-46f9-91cd-d51617ab7986",
   "metadata": {},
   "source": [
    "If we have two identical cameras, at points $$O$$ and $$O'$$ at a distance $$B$$ from each other, with focal length $$f$$, we can calculate the distance ($$Z$$) to object $$X$$ by using the *disparity* between where the object $$X$$ appears in the *left* image ($$x$$) and where it appears in the *right* image ($$x'$$).  \n",
    "\n",
    "In this simple case, the relation between disparity and distance is simply:\n",
    "\n",
    "\\begin{equation}\n",
    "disparity = x - x' = \\frac{Bf}{Z}\n",
    "\\end{equation}\n",
    "\n",
    "If we know $$B$$ an $$f$$, then we can rearrange this to give us distance as a function of disparity: \n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\frac{Bf}{x - x'}\n",
    "\\end{equation}\n",
    "\n",
    "You might notice that in case the disparity is zero, you will have an undefined result. This is just due to the fact that in this case the cameras are pointing in parallel, so in principle a disparity of zero should not be possible.   \n",
    "\n",
    "The general case is more complicated, but we will focus on this simple setup for now.  \n",
    "\n",
    "We can define the function as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c84144-64d8-4979-b5ff-89451ff2b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "function distance_from_disparity(B, f, disparity)\n",
    "    B*f/disparity\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6fe48b88-2ff7-4987-b8dd-79959ccfb5d1",
   "metadata": {},
   "source": [
    "Where $$B$$ and $$disparity$$ are measured in pixels, and $$f$$ is measured in centimeters. \n",
    "\n",
    "There is an inverse relation between distance and disparity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bef2a-acaf-41a6-b33d-6890cf8547d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "\n",
    "disparities = range(1, 50, length=50)\n",
    "distances = distance_from_disparity.(100, 0.1, disparities)\n",
    "\n",
    "plot(disparities, distances, label=\"Distance [cm]\")\n",
    "xlabel!(\"Disparity\")\n",
    "ylabel!(\"Distance [cm]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0aca1843-510f-46fa-9c68-73adb3c72f45",
   "metadata": {},
   "source": [
    "So once we have a disparity, it's relatively straightforward to get a distance. But how do we find disparities? \n",
    "\n",
    "## Disparity maps\n",
    "\n",
    "We usually represent the disparities for a given pair of images as a *disparity map*, which is an array with the same dimensions as (one of) your images, but with disparity values for each pixel. \n",
    "\n",
    "In principle, this is a two-dimensional problem, as an object might be matched to a point that has both a horizontal and vertical shift, but luckily, you can always find a transformation to turn this into a one dimensional problem.  \n",
    "\n",
    "The cartoon below illustrates what a disparity map might look like: \n",
    "![Own work](../images/dispairity_block_julia/disparity_cartoon.png)\n",
    "\n",
    "Above, we calculate the disparity with respect to the left image (you can do it with respect to the right image as well), and as you can see the disparity map tells us how many pixels to the right each object shifted to the left image vs the right image.  \n",
    "\n",
    "For a set of images (taken from the [Middlebury Stereo Datasets](https://vision.middlebury.edu/stereo/data/)):  \n",
    "![https://vision.middlebury.edu/stereo/eval/newEval/tsukuba/](../images/dispairity_block_julia/im3.png) ![https://vision.middlebury.edu/stereo/eval/newEval/tsukuba/](../images/dispairity_block_julia/im4.png)   \n",
    "\n",
    "The corresponding disparity map can be visualized as follows:   \n",
    "\n",
    "![https://vision.middlebury.edu/stereo/eval/newEval/tsukuba/](../images/dispairity_block_julia/groundtruth.png) \n",
    "\n",
    "With darker pixels having lower disparity values, and brighter pixels having higher disparity values, meaning the dark objects are far away from the cameras, while the bright ones are close.   \n",
    "\n",
    "The ground truth disparity as shown above is usually calculated from [LiDAR](https://en.wikipedia.org/wiki/Lidar) or some other accurate method, and our goal is to get as close as possible to those values using only the images above. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c53e2252-c003-4088-862a-fca7fab0804d",
   "metadata": {},
   "source": [
    "## A naive approach\n",
    "\n",
    "So let's try and calculate disparity for the images above.   \n",
    "There are many, many approaches to calculating disparity, but let us begin with the most simple approach we can think of.   \n",
    "As a start, let us go through each row of pixels in the left image, and for that pixel, try and find the most similar pixel in the right image. \n",
    "\n",
    "So let us try and take the squared difference between pixels values as our similarity metric.\n",
    "As we are going to be doing the same thing for every row of pixels, we are just going to define a function that does the basic logic, and then apply the same function to every case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9aba32b-17e5-44a1-8015-70a280160609",
   "metadata": {},
   "outputs": [],
   "source": [
    "function smallest_diff(left_pixel, row, metric)\n",
    "\n",
    "    disparity_candidates = metric.(left_pixel, row)\n",
    "\n",
    "    # Minus one as Julia counts from 1\n",
    "    argmin(disparity_candidates) -1\n",
    "    \n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6310819-2ad5-43d1-a339-31387f3ae54b",
   "metadata": {},
   "source": [
    "Let's define a distance metric as the squared distance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada3e93e-732b-4130-8506-a1a371026eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_difference = (x, y) -> (x-y)^2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c30074e-6b1b-411b-80e6-c3fd1cd411af",
   "metadata": {},
   "source": [
    "And as a test case let's create the cartoon image we had above (inverted, for clarity): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9ff00a-df5b-4340-a030-ffc303e7823e",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_image = zeros(Float64, (8, 8))\n",
    "right_image = zeros(Float64, (8, 8))\n",
    "disparity = zeros(UInt8, (8, 8))\n",
    "\n",
    "left_image[2, 1] = 1\n",
    "left_image[3, 4:8] = [1 1 1 1 1]\n",
    "\n",
    "right_image[7, 1] = 1\n",
    "right_image[4, 4] = 1\n",
    "right_image[5, 5] = 1\n",
    "right_image[6, 6] = 1\n",
    "right_image[7, 7] = 1\n",
    "right_image[8, 8] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebe3b48-f374-4c6f-95c7-273f3bd96abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(left_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566581ca-3bd6-42f2-ba4e-3b7e346475a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(right_image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c97fa0-dc0c-4a8f-95b6-ce6f90f472cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, size(left_image)[1])\n",
    "    for j in range(1, size(left_image)[2])\n",
    "        disparity[j, i] = max(smallest_diff(left_image[j, i], right_image[j:end, i], squared_difference), 1)\n",
    "    end\n",
    "end\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec469a8c-6bcc-4382-adc7-bebda5871d08",
   "metadata": {},
   "source": [
    "So how did we do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b0682-45d3-4987-9c8c-4a3372e2b9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(disparity'/5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c67f2cd-3530-462d-af23-e14be445bae9",
   "metadata": {},
   "source": [
    "So the toy example works! We'll start with the example case above, but for simplicity we'll stick to grayscale at first: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb06325-0931-4ed5-93a5-2970de602568",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Images, FileIO\n",
    "\n",
    "tsukuba_left_gray = Gray.(load(\"../images/dispairity_block_julia/im3.png\"))\n",
    "tsukuba_right_gray = Gray.(load(\"../images/dispairity_block_julia/im4.png\"))\n",
    " \n",
    "\n",
    "tsukuba_left_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a0d0ad-9d99-4a53-9efa-129f2af8b642",
   "metadata": {},
   "outputs": [],
   "source": [
    "function pixel_match(left_image, right_image, metric, max_disp)\n",
    "\n",
    "    num_rows = size(left_image)[1]\n",
    "    num_cols = size(left_image)[2]\n",
    "\n",
    "    disparity = zeros(size(left_image))\n",
    "    for i in range(1, num_rows)\n",
    "        for j in range(1, num_cols)\n",
    "            terminator = min(j+max_disp, num_cols)\n",
    "            disparity[i, j] = max(smallest_diff(left_image[i, j], right_image[i, j:terminator], metric), 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    disparity\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865ca13-7cf4-4076-875b-8e37ba67c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsukuba_disparity_gray = pixel_match(tsukuba_left_gray, tsukuba_right_gray,  squared_difference, 50);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74dcf93d-8187-4132-a466-9f4acc198344",
   "metadata": {},
   "source": [
    "So let's see how we did?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d886334-d7ee-4af0-8abf-4cbb5aee61c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(tsukuba_disparity_gray / maximum(tsukuba_disparity_gray)   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e91ab87-f98a-4ff3-9ac1-1ba1ce79734d",
   "metadata": {},
   "source": [
    "Looking at the predicted disparity, we can see there is some vague resemblance to the input image, but we're still pretty far from the target: ![https://vision.middlebury.edu/stereo/eval/newEval/tsukuba/](../images/dispairity_block_julia/groundtruth.png) \n",
    "\n",
    "\n",
    "As you can imagine, we are only comparing single channel pixels values, and it's very likely that we might just find a better match by chance. In grayscale we are only matching pixel intensity, and we have no idea whether something is bright green, or bright red. \n",
    "\n",
    "So let's try and improve the odds of a good match by adding colour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb55c6-c364-4401-a247-e1200f7bab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsukuba_left_rgb = load(\"../images/dispairity_block_julia/im3.png\")\n",
    "tsukuba_right_rgb = load(\"../images/dispairity_block_julia/im4.png\")\n",
    " \n",
    "\n",
    "tsukuba_left_rgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51380435-757b-4886-a3b6-feabafc11417",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_difference_rgb = (x, y) -> ((x.b-y.b)^2 + (x.g-y.g)^2 + (x.r-y.r)^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bede50-ee39-4449-ab56-4f815514e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pix = tsukuba_left_rgb[150, 200] - tsukuba_right_rgb[150, 200]\n",
    "print(Gray(pix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e12425-3cfe-44e8-90ed-b22bdf3aba42",
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_difference_rgb(tsukuba_left_rgb[150, 200], tsukuba_right_rgb[150, 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf37f22-c229-462b-8a55-e17fd8f5622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsukuba_disparity_rgb = pixel_match(tsukuba_left_rgb, tsukuba_right_rgb,  squared_difference_rgb, 50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be0664f-9239-4b59-99a2-647e901c8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(tsukuba_disparity_rgb / maximum(tsukuba_disparity_rgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049242dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "asdasd \n",
    "\n",
    "\n",
    "asdasd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8296f976-b5b1-443a-a8e0-5a15a3888440",
   "metadata": {},
   "source": [
    "So, not much of an improvement.  \n",
    "\n",
    "Can we do better? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "134a47b2-78f7-40b1-adf9-f56751a6058e",
   "metadata": {},
   "source": [
    "## Block matching\n",
    "\n",
    "The obvious downside of the naive approach above is that it only ever looks at one pixel (in each image) at a time.  \n",
    "That's not a lot of information, and also not how we intuitively match objects.   \n",
    "\n",
    "Look at the image below. Can you guess the best match for the pixel in the row of pixels below it? \n",
    "\n",
    "![Own work](../images/dispairity_block_julia/pixel_match.png)\n",
    "\n",
    "\n",
    "Given only this information, it's impossible for us to guess whether the green pixel matches with the pixels at location 3, 5 or 7.  \n",
    "\n",
    "If however I was to give you more context, i.e. a block of say 3x3 pixels, would this make things simpler? \n",
    "\n",
    "![Own work](../images/dispairity_block_julia/block_match.png)\n",
    "\n",
    "In this case, there is an unambiguous answer, which is the principle behind block-matching. \n",
    "\n",
    "We can modify the function `pixel_match` above to instead match blocks of pixels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2253616-58ef-4f6d-b5be-0ddc76e95600",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squared_difference_rgb_block = (x, y) -> sum((channelview(x) - channelview(y)).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906ccba-2a54-4bb7-9ddc-87a8f0b35276",
   "metadata": {},
   "outputs": [],
   "source": [
    "function smallest_block_diff(left_block, row, metric, block_size)\n",
    "\n",
    "    view_length = size(row)[2]\n",
    "    disparity_candidates = zeros(view_length - 2*block_size)\n",
    "\n",
    "    for i in range(block_size + 1, view_length - block_size )\n",
    "        \n",
    "        disparity_candidates[i-block_size] = metric(left_block, row[:, i-block_size:i+block_size])\n",
    "    end\n",
    "    limit = block_size^2 + block_size\n",
    "    # Minus one as Julia counts from 1\n",
    "    \n",
    "    minval = minimum(disparity_candidates)\n",
    "    if  minval < limit\n",
    "        return argmin(disparity_candidates) -1\n",
    "    else\n",
    "        return 0\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306c665-c136-4b30-81e0-fb1b1a199437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6463e4e4-7bd5-45e5-8185-bec3bb637021",
   "metadata": {},
   "outputs": [],
   "source": [
    "function block_match(left_image, right_image, block_size, metric, max_disp)\n",
    "\n",
    "    \n",
    "    num_rows = size(left_image)[1]\n",
    "    num_cols = size(left_image)[2]\n",
    "\n",
    "    disparity = zeros(size(left_image))\n",
    "    for i in range(block_size + 1, num_rows - block_size)\n",
    "        for j in range(block_size + 1, num_cols - block_size)\n",
    "            terminator = min(j+max_disp, num_cols)\n",
    "\n",
    "            row_block_index = i-block_size:i+block_size\n",
    "            col_block_index = j-block_size:j+block_size\n",
    "            \n",
    "            disparity[i, j] = max(smallest_block_diff(left_image[row_block_index, col_block_index], right_image[row_block_index, j-block_size:terminator], metric, block_size), 1)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    disparity\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c2fdbc-b9b7-49c4-a79a-73fbb908005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsukuba_disparity_rgb = block_match(tsukuba_right_rgb, tsukuba_left_rgb, 5,  sum_squared_difference_rgb_block, 30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb4c6c-c2da-4d1b-a08e-da627817ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gray.(tsukuba_disparity_rgb / (maximum(tsukuba_disparity_rgb)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d4659-b125-4d6e-8e9e-d2e8145913cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum(tsukuba_disparity_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896773a1-a060-4bd2-94bc-8f8e7b1585be",
   "metadata": {},
   "outputs": [],
   "source": [
    "function smallest_block_diff(left_block, row, metric, block_size)\n",
    "\n",
    "    view_length = size(row)[2]\n",
    "    disparity_candidates = zeros(view_length - 2*block_size)\n",
    "\n",
    "    for i in range(block_size + 1, view_length - block_size )\n",
    "        \n",
    "        disparity_candidates[i-block_size] = metric(left_block, row[:, i-block_size:i+block_size])\n",
    "    end\n",
    "\n",
    "    return disparity_candidates\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ce3e82-cc56-478f-a9ec-6e075dc4f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 5\n",
    "i = 100\n",
    "j = 220\n",
    "x = tsukuba_left_rgb[i-block_size:i+block_size, j-block_size:end]#\n",
    "y = tsukuba_right_rgb[i-block_size:i+block_size, j-block_size:j+block_size]#end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623adc76-527a-443e-8c21-8074893b340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c24e15b-89a9-4821-8e4a-addde0d8c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs = smallest_block_diff(y, x, sum_squared_difference_rgb_block, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5900b4db-3b44-4303-8f2e-ee012f507d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf088ff5-cabb-4f5a-baa9-b2871e4d7a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "160 - argmin(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18bb68-a89e-4a09-a3e3-5e165dffaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum(diffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e0563-014c-487e-8b13-7aa1c2b846db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, index-block_size:index+block_size] - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c541d-ac37-4d6c-9689-7cc070660776",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af39e25e-4f36-44e9-9e77-1ec024420877",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe13e14b-44c1-4eb1-b3f1-8b92bf6b4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, index-block_size:index+block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0263769-4cd1-4f31-8913-3a928c0b4e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelview(x[:, index-block_size:index+block_size]) #- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14273454-ba57-4397-909f-01fc2112cad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelview(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e0dfa-d88d-472f-991e-f8d2cd2664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "channelview(x[:, index-block_size:index+block_size]) - channelview(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d8bbff-c348-407e-b378-02b51e29e6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_squared_difference_rgb_block(x[:, index-block_size:index+block_size], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2448b7-63e5-4af0-a3ca-089c0611f9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = i-block_size:i+block_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac755d6-762d-4b8c-9061-6301bf84d302",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"images/groundtruth.png\"\n",
    "img = load(img_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
